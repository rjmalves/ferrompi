rd_("AjContains the success valueBm<code>MPI_ERR_OP</code> \xe2\x80\x94 invalid operationCa8-bit unsigned integer (<code>MPI_UINT8_T</code>)Bm<code>MPI_ERR_ARG</code> \xe2\x80\x94 invalid argumentAhContains the error valueBn32-bit floating point (<code>MPI_FLOAT</code>)Bo64-bit floating point (<code>MPI_DOUBLE</code>)C`32-bit signed integer (<code>MPI_INT32_T</code>)C`64-bit signed integer (<code>MPI_INT64_T</code>)mMaximum valuemMinimum valueClMPI error with class, code, and descriptive message from \xe2\x80\xa6AgMPI environment handle.CdUnrecognized error class from the MPI implementationmSum of valuesCfThe datatype tag used for FFI dispatch to the C layer.Ca<code>MPI_ERR_TAG</code> \xe2\x80\x94 invalid tag argumentCc32-bit unsigned integer (<code>MPI_UINT32_T</code>)Cc64-bit unsigned integer (<code>MPI_UINT64_T</code>)Bk<code>MPI_ERR_WIN</code> \xe2\x80\x94 invalid windowBdGet the value associated with a key.BcCreate a new empty MPI info object.BiSet a key-value pair on this info object.AcTag of the message.Cb<code>MPI_ERR_COMM</code> \xe2\x80\x94 invalid communicatorCh<code>MPI_ERR_DIMS</code> \xe2\x80\x94 invalid dimension argumentCa<code>MPI_ERR_FILE</code> \xe2\x80\x94 invalid file handleCa<code>MPI_ERR_INFO</code> \xe2\x80\x94 invalid info objectCgAn MPI info object for passing hints to MPI operations.AaProduct of valuesBj<code>MPI_ERR_RANK</code> \xe2\x80\x94 invalid rankBj<code>MPI_ERR_ROOT</code> \xe2\x80\x94 invalid rootCg<code>MPI_ERR_TYPE</code> \xe2\x80\x94 invalid datatype argumentAgThe raw MPI error code.AoReturns the argument unchanged.000000000000000BlInitialize MPI with single-threaded support.BaCalls <code>U::from(self)</code>.000000000000000CcLock a specific rank\xe2\x80\x99s window (passive target \xe2\x80\xa6CeGet a handle representing <code>MPI_INFO_NULL</code>.CiGet the rank of the calling process in this communicator.BoReceive a slice of values from another process.BbInclusive prefix reduction (scan).BjSend a slice of values to another process.CaGet the number of processes in this communicator.CeTest if the operation has completed without blocking.CfTest if this operation has completed without blocking.BcWait for the operation to complete.BdWait for this operation to complete.Ce<code>MPI_ERR_COUNT</code> \xe2\x80\x94 invalid count argumentAoError types for MPI operations.Bl<code>MPI_ERR_GROUP</code> \xe2\x80\x94 invalid groupBj<code>MPI_ERR_OTHER</code> \xe2\x80\x94 other errorCdAbort MPI execution across all processes in this \xe2\x80\xa6DdCheck an MPI return code, returning <code>Ok(())</code> for success.BdThe error class (category of error).CeNumber of elements in the message (determined via \xe2\x80\xa6BfFence synchronization (active target).C`Flush pending RMA operations to the locked rank.C`Flush pending RMA operations to a specific rank.AdNonblocking receive.BnNonblocking inclusive prefix reduction (scan).AaNonblocking send.BgBlocking probe for an incoming message.BdSLURM scheduler environment helpers.CkSplit this communicator into sub-communicators based on \xe2\x80\xa6AoStart the persistent operation.BlGet a handle to <code>MPI_COMM_WORLD</code>.B`Get the current wall-clock time.Cf<code>MPI_ERR_BUFFER</code> \xe2\x80\x94 invalid buffer pointerCb<code>MPI_ERR_INTERN</code> \xe2\x80\x94 internal MPI errorAoResult type for MPI operations.ClSplit by shared memory domain (same physical node). Maps \xe2\x80\xa6BnShared access (multiple shared locks allowed).AnOnly single-threaded executionCcInformation about a probed or received MPI message.BdExclusive prefix reduction (exscan).BbGather values to the root process.BjNonblocking probe for an incoming message.AeGet the SLURM job ID.BbReduce values to the root process.AkSource rank of the message.C`<code>MPI_ERR_PENDING</code> \xe2\x80\x94 pending requestCg<code>MPI_ERR_REQUEST</code> \xe2\x80\x94 invalid request handleBhA handle to a nonblocking MPI operation.Be<code>MPI_SUCCESS</code> \xe2\x80\x94 no errorBn<code>MPI_ERR_UNKNOWN</code> \xe2\x80\x94 unknown errorAhBarrier synchronization.CdGather variable amounts of data to the root process.C`Nonblocking exclusive prefix reduction (exscan).AkNonblocking gather to root.AkNonblocking reduce to root.D`Human-readable error message from <code>MPI_Error_string</code>.BjScatter values from root to all processes.BcGet the MPI library version string.CcMulti-threaded, but MPI calls only from main threadCj<code>MPI_ERR_IN_STATUS</code> \xe2\x80\x94 error code is in statusAhInternal ferrompi error.BeLock type for window synchronization.AkFull multi-threaded supportAdReduction operationsCb<code>MPI_ERR_TOPOLOGY</code> \xe2\x80\x94 invalid topologyCc<code>MPI_ERR_TRUNCATE</code> \xe2\x80\x94 message truncatedB`Allocate a shared memory window.BfAll-to-all personalized communication.hferrompiCfMap a raw MPI error class integer to the enum variant.AdNonblocking barrier.CdNonblocking gather variable amounts of data to root.AnNonblocking scatter from root.CkLock all ranks\xe2\x80\x99 windows (passive target synchronization).CgScatter variable amounts of data from the root process.AfBlocking send-receive.BoWait for all persistent operations to complete.CbWait for all requests in a collection to complete.BjExclusive access (no other locks allowed).BiRAII guard for a single-rank window lock.CfSplit types for <code>Communicator::split_type</code>.C`Constant for opting out of a communicator split.C`All-gather values (gather and broadcast to all).CgAll-reduce values (reduce and broadcast result to all).B`All-to-all with variable counts.BmBroadcast a slice from root to all processes.CkGet the number of processes in the window\xe2\x80\x99s communicator.AlDuplicate this communicator.BnFlush all pending RMA operations to all ranks.CaCreate a structured error from an MPI error code.CbNonblocking all-to-all personalized communication.CnCheck if this request is currently active (started but not \xe2\x80\xa6CgNonblocking scatter variable amounts of data from root.CnGet the compact node list string (e.g., \xe2\x80\x9cnode[001-004]\xe2\x80\x9d\xe2\x80\xa6ClGet the name of the compute node this process is running \xe2\x80\xa6BhGet the total number of nodes allocated.CmInitialize a persistent scan (inclusive prefix reduction) \xe2\x80\xa6BeStart multiple persistent operations.C`Multi-threaded, but MPI calls serialized by userCmAll-gather variable amounts of data (gather and broadcast \xe2\x80\xa6BlInitialize a persistent broadcast operation.AgNonblocking all-gather.AgNonblocking all-reduce.BlNonblocking all-to-all with variable counts.AfNonblocking broadcast.C`Get the local (intra-node) rank of this process.BaGet the number of tasks per node.CcGet the raw info handle for passing to C functions.BnGet the raw request handle (for advanced use).0AnGet the raw MPI window handle.CcGet the raw communicator handle (for advanced use).B`Split this communicator by type.CkTag values matching C-side <code>FERROMPI_*</code> defines.CiTrait for types that can be used in MPI communication \xe2\x80\xa6AiMPI thread support levelsClInitialize a persistent exclusive scan (exclusive prefix \xe2\x80\xa6BiInitialize a persistent gather operation.C`Nonblocking all-gather variable amounts of data.CgInitialize MPI with the specified thread support level.CgGet a slice of this process\xe2\x80\x99s local shared memory \xe2\x80\xa6BiInitialize a persistent reduce operation.BhInclusive scan of a single scalar value.AdAn MPI communicator.BfRAII guard for a lock-all window lock.CeOperation not supported (e.g., MPI 4.0 persistent \xe2\x80\xa6ClA shared memory window allocated across processes on the \xe2\x80\xa6CmInitialize a persistent gatherv operation (variable-count \xe2\x80\xa6BiCheck if this request has been completed.B`Check if MPI has been finalized.BkCheck if running under SLURM job scheduler.BlQuery another rank\xe2\x80\x99s shared memory region.BjInitialize a persistent scatter operation.CnCreate a communicator containing only processes that share \xe2\x80\xa6BoGet the thread support level that was provided.ChInvalid buffer provided (e.g., send/recv buffer size \xe2\x80\xa6CdMPI error class, categorizing the type of MPI error.BmInitialize a persistent all-to-all operation.B`Get the number of CPUs per task.BhExclusive scan of a single scalar value.CaReduce a single scalar value to the root process.CnInitialize a persistent scatterv operation (variable-count \xe2\x80\xa6BmInitialize a persistent all-gather operation.BmInitialize a persistent all-reduce operation.CaInitialize a persistent all-to-allv operation \xe2\x80\xa6BbCheck if MPI has been initialized.BhGet the processor name for this process.BdIn-place reduce to the root process.CaInitialize a persistent all-gatherv operation \xe2\x80\xa6ChGet a mutable slice of this process\xe2\x80\x99s local shared \xe2\x80\xa6BaAll-reduce a single scalar value.B`A persistent MPI request handle.AkAll-reduce values in place.BaMPI has already been initialized.BgReduce-scatter with uniform block size.CcNonblocking reduce-scatter with uniform block size.CfInitialize a persistent in-place all-reduce operation.CgInitialize a persistent reduce-scatter-block operation.")